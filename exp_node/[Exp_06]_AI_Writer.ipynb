{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 프로젝트 6 멋진 작사가 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. 라이브러리 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_tM6leEhCTm",
        "outputId": "9f2aee1c-7ef6-4f43-ec3f-0b4252671a6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.8.2\n"
          ]
        }
      ],
      "source": [
        "import glob  #glob 모듈의 glob 함수는 사용자가 제시한 조건에 맞는 파일명을 리스트 형식으로 반환한다\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os, re\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. 학습할 데이터 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXiVGmoAjJFm",
        "outputId": "5f92d083-60f8-4a7b-bd46-8408ac85d532"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "데이터 크기: 187088\n",
            "Examples:\n",
            " ['[Hook]', \"I've been down so long, it look like up to me\", 'They look up to me']\n"
          ]
        }
      ],
      "source": [
        "txt_file_path = \"/content/drive/MyDrive/data/lyrics/lyrics/*\" #os.getenv(x)함수는 환경 변수x의 값을 포함하는 문자열 변수를 반환합니다. txt_file_path 에 \"/root/aiffel/lyricist/data/lyrics/*\" 저장\n",
        "\n",
        "txt_list = glob.glob(txt_file_path) #txt_file_path 경로에 있는 모든 파일명을 리스트 형식으로 txt_list 에 할당\n",
        "\n",
        "raw_corpus = [] \n",
        "\n",
        "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
        "for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "        raw = f.read().splitlines() #read() : 파일 전체의 내용을 하나의 문자열로 읽어온다. , splitlines()  : 여러라인으로 구분되어 있는 문자열을 한라인씩 분리하여 리스트로 반환\n",
        "        raw_corpus.extend(raw) # extend() : 리스트함수로 추가적인 내용을 연장 한다.\n",
        "\n",
        "print(\"데이터 크기:\", len(raw_corpus))\n",
        "print(\"Examples:\\n\", raw_corpus[:3])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o58OQVl3Atd2",
        "outputId": "1d0627b1-db79-44bc-db4b-00f26345833b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<start> this is sample sentence . <end>\n"
          ]
        }
      ],
      "source": [
        "# 입력된 문장을\n",
        "#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
        "#     2. 특수문자 양쪽에 공백을 넣고\n",
        "#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
        "#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
        "#     5. 다시 양쪽 공백을 지웁니다\n",
        "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip()\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
        "    sentence = sentence.strip()\n",
        "    sentence = '<start> ' + sentence + ' <end>'\n",
        "    return sentence\n",
        "\n",
        "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2wgWRYDhJ8G",
        "outputId": "d725c1db-e901-4e92-e998-d2d7e9bcc610"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<start> hook <end>',\n",
              " '<start> i ve been down so long , it look like up to me <end>',\n",
              " '<start> they look up to me <end>',\n",
              " '<start> i got fake people showin fake love to me <end>',\n",
              " '<start> straight up to my face , straight up to my face <end>',\n",
              " '<start> i ve been down so long , it look like up to me <end>',\n",
              " '<start> they look up to me <end>',\n",
              " '<start> i got fake people showin fake love to me <end>',\n",
              " '<start> straight up to my face , straight up to my face verse <end>',\n",
              " '<start> somethin ain t right when we talkin <end>']"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus = []\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "    \n",
        "    if len(sentence) == 0: continue\n",
        "    if sentence[-1] == \":\": continue\n",
        "    \n",
        "    preprocessd_sentence = preprocess_sentence(sentence)\n",
        "    if len(preprocessd_sentence.split(' ')) > 15: continue\n",
        "    corpus.append(preprocessd_sentence)\n",
        "    \n",
        "corpus[ : 10]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. 모델 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "UsjrBMJZBv3G"
      },
      "outputs": [],
      "source": [
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "dQaT0Z4RBcpX"
      },
      "outputs": [],
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=15000,\n",
        "        filters=' ',\n",
        "        oov_token='<unk>'\n",
        "    )\n",
        "\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "tensor = tokenizer.texts_to_sequences(corpus)\n",
        "tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "src_input = tensor[:, :-1]\n",
        "tgt_input = tensor[:, 1:]\n",
        "\n",
        "VOCAB_SIZE = tokenizer.num_words + 1   \n",
        "\n",
        "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=12)\n",
        "\n",
        "BUFFER_SIZE = len(enc_train)\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "embedding_size = 512\n",
        "hidden_size = 1024\n",
        "lyricist = TextGenerator(tokenizer.num_words + 1, embedding_size, hidden_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- num_words를 15000으로 늘림.  \n",
        "- embedding_size와 hidden_size를 조정함.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmnRCj0jNq_O",
        "outputId": "40c02740-f39b-46f7-bf0e-de868adc40da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 : <unk>\n",
            "2 : <start>\n",
            "3 : <end>\n",
            "4 : i\n",
            "5 : ,\n",
            "6 : the\n",
            "7 : you\n",
            "8 : and\n",
            "9 : a\n",
            "10 : to\n"
          ]
        }
      ],
      "source": [
        "for idx in tokenizer.index_word:\n",
        "    print(idx, \":\", tokenizer.index_word[idx])\n",
        "\n",
        "    if idx >= 10: break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOvSxDjqEQD3",
        "outputId": "68a5fa24-4733-433e-cf15-69da9916df58"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(256, 14, 15001), dtype=float32, numpy=\n",
              "array([[[-1.37694122e-04,  3.64059597e-05,  6.41370134e-04, ...,\n",
              "         -1.33131063e-04, -2.37313448e-04, -1.96396446e-04],\n",
              "        [-1.61555756e-04, -5.66811214e-05,  6.31674193e-04, ...,\n",
              "         -1.35644019e-04, -5.57060179e-04, -2.05239703e-04],\n",
              "        [ 9.96904928e-05, -3.64086591e-05,  1.54908776e-04, ...,\n",
              "         -8.93613178e-05, -4.07557003e-04, -2.93054909e-04],\n",
              "        ...,\n",
              "        [-8.98867322e-04, -6.79226476e-04,  3.90163448e-04, ...,\n",
              "         -9.41253733e-04,  6.05904963e-04, -8.18234752e-04],\n",
              "        [-9.90417087e-04, -3.84746876e-04,  2.12086918e-04, ...,\n",
              "         -7.70659593e-04,  9.06850619e-04, -1.00526831e-03],\n",
              "        [-1.05542596e-03, -4.25217440e-05, -2.08703568e-06, ...,\n",
              "         -5.79724845e-04,  1.18510798e-03, -1.16411573e-03]],\n",
              "\n",
              "       [[-1.37694122e-04,  3.64059597e-05,  6.41370134e-04, ...,\n",
              "         -1.33131063e-04, -2.37313448e-04, -1.96396446e-04],\n",
              "        [-2.23241019e-04, -1.09976863e-05,  7.74544314e-04, ...,\n",
              "         -3.55855591e-04, -3.42776708e-04, -7.37986062e-04],\n",
              "        [-5.06494136e-04, -2.28175661e-04,  7.18192081e-04, ...,\n",
              "         -4.57855291e-04, -3.66694818e-04, -1.17791200e-03],\n",
              "        ...,\n",
              "        [-1.77084655e-03,  2.13759439e-03, -5.61521039e-04, ...,\n",
              "          1.06460007e-03,  8.66761140e-04, -9.64660954e-04],\n",
              "        [-1.93442788e-03,  2.34190491e-03, -7.65306526e-04, ...,\n",
              "          1.05300709e-03,  9.79258097e-04, -1.01694162e-03],\n",
              "        [-2.11072038e-03,  2.51033902e-03, -9.47427179e-04, ...,\n",
              "          1.04026846e-03,  1.06788240e-03, -1.03909755e-03]],\n",
              "\n",
              "       [[-1.37694122e-04,  3.64059597e-05,  6.41370134e-04, ...,\n",
              "         -1.33131063e-04, -2.37313448e-04, -1.96396446e-04],\n",
              "        [ 1.29391410e-05,  3.67795554e-04,  5.10201557e-04, ...,\n",
              "         -5.32833219e-04, -2.19024601e-04,  2.21182563e-05],\n",
              "        [ 3.23345768e-04,  8.33490980e-04, -1.70064159e-05, ...,\n",
              "         -1.03194080e-03, -1.12182977e-04,  4.96116991e-04],\n",
              "        ...,\n",
              "        [-6.52894902e-04,  1.62496523e-03, -3.09117895e-04, ...,\n",
              "         -2.41918024e-04,  1.10952393e-03, -1.81241310e-04],\n",
              "        [-9.48736211e-04,  1.80210697e-03, -3.28780879e-04, ...,\n",
              "         -8.40654830e-05,  1.12871744e-03, -3.79646546e-04],\n",
              "        [-1.24163507e-03,  1.98382093e-03, -3.77912016e-04, ...,\n",
              "          4.08122432e-05,  1.13512715e-03, -5.30869933e-04]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[-1.37694122e-04,  3.64059597e-05,  6.41370134e-04, ...,\n",
              "         -1.33131063e-04, -2.37313448e-04, -1.96396446e-04],\n",
              "        [-4.98321169e-05,  1.53592715e-04,  7.24433165e-04, ...,\n",
              "          1.67070539e-07,  5.27231241e-05, -3.23898799e-04],\n",
              "        [ 9.33191841e-05,  2.39681802e-04,  7.75780063e-04, ...,\n",
              "          1.39230440e-04,  4.28975502e-04, -5.30276797e-04],\n",
              "        ...,\n",
              "        [ 1.66072545e-03, -9.39018209e-05, -1.11984881e-03, ...,\n",
              "          2.82388879e-04,  1.91157844e-04, -1.01018324e-03],\n",
              "        [ 1.44528202e-03,  1.09206252e-04, -1.09646725e-03, ...,\n",
              "          3.10820818e-04,  4.97975154e-04, -1.15539960e-03],\n",
              "        [ 1.16577523e-03,  4.22876008e-04, -1.10996165e-03, ...,\n",
              "          4.02863254e-04,  7.80421367e-04, -1.27557770e-03]],\n",
              "\n",
              "       [[-1.37694122e-04,  3.64059597e-05,  6.41370134e-04, ...,\n",
              "         -1.33131063e-04, -2.37313448e-04, -1.96396446e-04],\n",
              "        [-3.14545090e-04,  2.85469996e-05,  1.03994296e-03, ...,\n",
              "         -3.22773965e-04, -4.39329364e-04, -1.70489831e-04],\n",
              "        [-7.57894304e-04, -1.55672198e-04,  1.21655664e-03, ...,\n",
              "         -6.12788950e-04, -7.51802581e-04, -8.65988113e-05],\n",
              "        ...,\n",
              "        [-1.48422795e-03,  9.32736264e-04,  8.39337765e-04, ...,\n",
              "         -1.28920050e-03,  8.16143700e-04, -2.47106946e-05],\n",
              "        [-1.41140493e-03,  1.29207317e-03,  5.58451226e-04, ...,\n",
              "         -9.73707647e-04,  1.21868099e-03, -2.81856279e-04],\n",
              "        [-1.37130916e-03,  1.61812676e-03,  2.82868976e-04, ...,\n",
              "         -6.60755904e-04,  1.55530660e-03, -5.24890726e-04]],\n",
              "\n",
              "       [[-1.37694122e-04,  3.64059597e-05,  6.41370134e-04, ...,\n",
              "         -1.33131063e-04, -2.37313448e-04, -1.96396446e-04],\n",
              "        [-6.65886968e-04, -1.57237737e-04,  9.37014585e-04, ...,\n",
              "         -9.31615941e-05, -4.39377967e-04, -1.53475790e-04],\n",
              "        [-1.07855292e-03, -1.17959455e-04,  6.27744012e-04, ...,\n",
              "          3.03434383e-04, -3.58372810e-04,  6.98788062e-05],\n",
              "        ...,\n",
              "        [-1.65083143e-03,  2.44604819e-03, -1.27069047e-03, ...,\n",
              "          9.32029216e-04,  1.49065244e-03, -7.74092681e-04],\n",
              "        [-1.79101969e-03,  2.56645912e-03, -1.37805741e-03, ...,\n",
              "          9.07152949e-04,  1.48809701e-03, -8.05269519e-04],\n",
              "        [-1.95907289e-03,  2.66109034e-03, -1.46821979e-03, ...,\n",
              "          8.91830423e-04,  1.46652770e-03, -8.20233836e-04]]],\n",
              "      dtype=float32)>"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for src_sample, tgt_sample in dataset.take(1): break\n",
        "\n",
        "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
        "lyricist(src_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fn90KvwkB95b",
        "outputId": "63f280af-d21d-4823-a8a3-f6cb348f3331"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "487/487 [==============================] - 6762s 14s/step - loss: 3.5623\n",
            "Epoch 2/10\n",
            "487/487 [==============================] - 6749s 14s/step - loss: 3.0409\n",
            "Epoch 3/10\n",
            "487/487 [==============================] - 6778s 14s/step - loss: 2.8648\n",
            "Epoch 4/10\n",
            "487/487 [==============================] - 6754s 14s/step - loss: 2.7362\n",
            "Epoch 5/10\n",
            "487/487 [==============================] - 6790s 14s/step - loss: 2.6271\n",
            "Epoch 6/10\n",
            "487/487 [==============================] - 6906s 14s/step - loss: 2.5269\n",
            "Epoch 7/10\n",
            "487/487 [==============================] - 6798s 14s/step - loss: 2.4334\n",
            "Epoch 8/10\n",
            "487/487 [==============================] - 6798s 14s/step - loss: 2.3436\n",
            "Epoch 9/10\n",
            "487/487 [==============================] - 6782s 14s/step - loss: 2.2575\n",
            "Epoch 10/10\n",
            "487/487 [==============================] - 6767s 14s/step - loss: 2.1748\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1137fd5c10>"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none') \n",
        "# 클래스 분류 문제에서 softmax 함수를 거치면 from_logits = False(default값),그렇지 않으면 from_logits = True.\n",
        "\n",
        "lyricist.compile(loss=loss, optimizer=optimizer)\n",
        "\n",
        "lyricist.fit(dataset, epochs=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "13RdUDOLehkY"
      },
      "outputs": [],
      "source": [
        "#모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행\n",
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20): #시작 문자열을 init_sentence 로 받으며 디폴트값은 <start> 를 받는다\n",
        "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence]) #텍스트 안의 단어들을 숫자의 시퀀스의 형태로 변환\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    # 단어 하나씩 예측해 문장을 만듭니다\n",
        "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
        "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
        "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
        "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다 (도달 하지 못하였으면 while 루프를 돌면서 다음 단어를 예측)\n",
        "    while True: #루프를 돌면서 init_sentence에 단어를 하나씩 생성성\n",
        "        # 1\n",
        "        predict = model(test_tensor) \n",
        "        # 2\n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
        "        # 3 \n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "        # 4 \n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated #최종적으로 모델이 생성한 문장을 반환"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. 검증"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZtDY1IKheOiI",
        "outputId": "ae7886c1-2eeb-4710-dde3-66690277c524"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> i love you , i love you <end> '"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_text(lyricist, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6. 회고\n",
        "\n",
        "- RNN의 구성을 확인하는 시간이었다. 시간순서에 따른 학습인데, 각 글자별 학습이 진행되어 학습시간이 엄청 길었다.  \n",
        "- 학습 중간 중간에 모델을 저장하여 불러오고 싶었는데 keras에서 제공하는 기본 모델이 아니다보니 저장이 되지 않았다.  \n",
        "- 분명히 논리적이고 통계적이겠으나, 아직은 테스트에 대한 결과를 가지고 검토를 할 수 없어서 이게 맞는가 싶다.  \n",
        "- loss는 2.2이하로 맞췄으나 밤새 돌린 결과가 \"<start> i love you , i love you <end>\" 이렇게 심플할수가..."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('pyenv_3912')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "e6debceb626036820d184549e25d059a55b6b8771e25bc8133db281d329c34fc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
