{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "oG0kf1tNXVH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnmCGGQqW8uQ"
      },
      "source": [
        "# 프로젝트 6 멋진 작사가 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEQES8JlW8uU"
      },
      "source": [
        "1. 라이브러리 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_tM6leEhCTm"
      },
      "outputs": [],
      "source": [
        "import glob  #glob 모듈의 glob 함수는 사용자가 제시한 조건에 맞는 파일명을 리스트 형식으로 반환한다\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os, re\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "# 랜덤시드를 설정함.\n",
        "random_seed = 500\n",
        "random.seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "tf.random.set_seed(random_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TrdyFZJW8uY"
      },
      "source": [
        "2. 학습할 데이터 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXiVGmoAjJFm"
      },
      "outputs": [],
      "source": [
        "txt_file_path = \"/content/drive/MyDrive/data/lyrics/lyrics/*\" #os.getenv(x)함수는 환경 변수x의 값을 포함하는 문자열 변수를 반환합니다. txt_file_path 에 \"/root/aiffel/lyricist/data/lyrics/*\" 저장\n",
        "\n",
        "txt_list = glob.glob(txt_file_path) #txt_file_path 경로에 있는 모든 파일명을 리스트 형식으로 txt_list 에 할당\n",
        "\n",
        "raw_corpus = [] \n",
        "\n",
        "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
        "for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "        raw = f.read().splitlines() #read() : 파일 전체의 내용을 하나의 문자열로 읽어온다. , splitlines()  : 여러라인으로 구분되어 있는 문자열을 한라인씩 분리하여 리스트로 반환\n",
        "        raw_corpus.extend(raw) # extend() : 리스트함수로 추가적인 내용을 연장 한다.\n",
        "\n",
        "print(\"데이터 크기:\", len(raw_corpus))\n",
        "print(\"Examples:\\n\", raw_corpus[:3])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o58OQVl3Atd2"
      },
      "outputs": [],
      "source": [
        "# 입력된 문장을\n",
        "#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
        "#     2. 특수문자 양쪽에 공백을 넣고\n",
        "#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
        "#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
        "#     5. 다시 양쪽 공백을 지웁니다\n",
        "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip()\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
        "    sentence = sentence.strip()\n",
        "    sentence = '<start> ' + sentence + ' <end>'\n",
        "    return sentence\n",
        "\n",
        "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2wgWRYDhJ8G"
      },
      "outputs": [],
      "source": [
        "corpus = []\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "    \n",
        "    if len(sentence) == 0: continue\n",
        "    if sentence[-1] == \":\": continue\n",
        "    \n",
        "    preprocessd_sentence = preprocess_sentence(sentence)\n",
        "    if len(preprocessd_sentence.split(' ')) > 15: continue\n",
        "    corpus.append(preprocessd_sentence)\n",
        "    \n",
        "corpus[ : 10]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk7h5QN4W8ua"
      },
      "source": [
        "3. 모델 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsjrBMJZBv3G"
      },
      "outputs": [],
      "source": [
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        # self.rnn_3 = tf.keras.layers.LSTM(1024, return_sequences=True)\n",
        "        # self.linear1 = tf.keras.layers.Dense(int(vocab_size*1.2))\n",
        "        self.linear2 = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        # out = self.rnn_3(out)\n",
        "        # out = self.linear1(out)\n",
        "        out = self.linear2(out)\n",
        "        \n",
        "        return out\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQaT0Z4RBcpX"
      },
      "outputs": [],
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=15000,\n",
        "        filters=' ',\n",
        "        oov_token='<unk>'\n",
        "    )\n",
        "\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "tensor = tokenizer.texts_to_sequences(corpus)\n",
        "tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "src_input = tensor[:, :-1]\n",
        "tgt_input = tensor[:, 1:]\n",
        "\n",
        "VOCAB_SIZE = tokenizer.num_words + 1   \n",
        "\n",
        "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=12)\n",
        "\n",
        "BUFFER_SIZE = len(enc_train)\n",
        "BATCH_SIZE = 256+ 256\n",
        "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
        "# dataset = dataset.shuffle(BUFFER_SIZE) # shuffle사용안함.\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "embedding_size = 2048 + 1024 + 1024 + 1024\n",
        "hidden_size = 1024 + 512 + 512\n",
        "lyricist = TextGenerator(tokenizer.num_words + 1, embedding_size, hidden_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ns-hfnaUW8uc"
      },
      "source": [
        "- num_words를 15000으로 늘림.  \n",
        "- embedding_size와 hidden_size를 조정함.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmnRCj0jNq_O"
      },
      "outputs": [],
      "source": [
        "for idx in tokenizer.index_word:\n",
        "    print(idx, \":\", tokenizer.index_word[idx])\n",
        "\n",
        "    if idx >= 10: break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOvSxDjqEQD3"
      },
      "outputs": [],
      "source": [
        "for src_sample, tgt_sample in dataset.take(1): break\n",
        "\n",
        "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
        "lyricist(src_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xre6fvA9W8ud"
      },
      "source": [
        "4. 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fn90KvwkB95b"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none') \n",
        "# 클래스 분류 문제에서 softmax 함수를 거치면 from_logits = False(default값),그렇지 않으면 from_logits = True.\n",
        "\n",
        "lyricist.compile(loss=loss, optimizer=optimizer)\n",
        "\n",
        "history = lyricist.fit(dataset, epochs=10, validation_data=(enc_val, dec_val))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uzIM2KQW8ue"
      },
      "outputs": [],
      "source": [
        "# loss 그래프로 표시.\n",
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss', linestyle='--')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13RdUDOLehkY"
      },
      "outputs": [],
      "source": [
        "#모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행\n",
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20): #시작 문자열을 init_sentence 로 받으며 디폴트값은 <start> 를 받는다\n",
        "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence]) #텍스트 안의 단어들을 숫자의 시퀀스의 형태로 변환\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    # 단어 하나씩 예측해 문장을 만듭니다\n",
        "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
        "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
        "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
        "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다 (도달 하지 못하였으면 while 루프를 돌면서 다음 단어를 예측)\n",
        "    while True: #루프를 돌면서 init_sentence에 단어를 하나씩 생성성\n",
        "        # 1\n",
        "        predict = model(test_tensor) \n",
        "        # 2\n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
        "        # 3 \n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "        # 4 \n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated #최종적으로 모델이 생성한 문장을 반환"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_en9WPPW8uf"
      },
      "source": [
        "5. 검증"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtDY1IKheOiI"
      },
      "outputs": [],
      "source": [
        "generate_text(lyricist, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DTLxuhRW8uf"
      },
      "source": [
        "6. 회고\n",
        "\n",
        "- RNN의 구성을 확인하는 시간이었다. 시간순서에 따른 학습인데, 각 글자별 학습이 진행되어 학습시간이 엄청 길었다.  \n",
        "- 학습 중간 중간에 모델을 저장하여 불러오고 싶었는데 keras에서 제공하는 기본 모델이 아니다보니 저장이 되지 않았다.  \n",
        "- 분명히 논리적이고 통계적이겠으나, 아직은 테스트에 대한 결과를 가지고 검토를 할 수 없어서 이게 맞는가 싶다.  \n",
        "- loss는 2.2이하로 내려가지 않음."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "private_outputs": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('pyenv_3912')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "e6debceb626036820d184549e25d059a55b6b8771e25bc8133db281d329c34fc"
      }
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}